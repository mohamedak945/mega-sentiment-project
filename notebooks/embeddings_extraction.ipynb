{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcbbc99e-3531-4db6-9309-c6eb1a135eac",
   "metadata": {},
   "source": [
    "# Step 0: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc517eea-1ba0-4558-acfb-33955bd66bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, MT5EncoderModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd2acd5-57c6-429d-80d2-69959aaac36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../p1_data/cleaned/amazon_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b05e94-c3c8-4cef-8dcf-6b4f6bbf72aa",
   "metadata": {},
   "source": [
    "# Step 1: Select Device (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df35783b-2abe-45df-be6e-4566ab033955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon Chip will be our device\n"
     ]
    }
   ],
   "source": [
    "def select_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU Nvidia cuda will be our device\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Apple Silicon Chip will be our device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CPU will be our device\")\n",
    "    \n",
    "    return device\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30a634-669e-4b3e-9a1e-c0f57bb6fb47",
   "metadata": {},
   "source": [
    "# Step 2: Get Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810b9b80-f222-4893-871c-ae8f036549d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model_and_tokenizer(model_name):\n",
    "    ## Model\n",
    "    # Select Model [for mt5 just import the encoder]\n",
    "    if \"mt5\" in model_name:\n",
    "        model = MT5EncoderModel.from_pretrained(model_name)\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    ####################################################\n",
    "    ## Tokenizer\n",
    "    # Select Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)    \n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad2af38-141d-47fc-8a54-83ab01856562",
   "metadata": {},
   "source": [
    "# Step 3: Tokenization and Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66265067-1a7f-4a92-9d2b-aef813dd928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_and_embedding(dataframe, model_name, batch_size=100):\n",
    "    # Starting timer\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Set model and tokenizer\n",
    "    tokenizer, model = select_model_and_tokenizer(model_name)\n",
    "    \n",
    "    # Convert dataframe's \"cleaned_review\" column to list of reviews\n",
    "    reviews_data = dataframe[\"cleaned_review\"].astype(str).to_list()\n",
    "    \n",
    "    \n",
    "    # ------------------- Tokenization and Embedding in batches ------------------------------------\n",
    "\n",
    "    # List of all embeddings from the batches\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Batcheting\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(reviews_data), batch_size), desc=\"Processing Batches\"):\n",
    "            batch_reviews = reviews_data[i: i+batch_size]\n",
    "\n",
    "\n",
    "    # Tokenizing each batch and save into \"inputs\"\n",
    "            inputs = tokenizer(\n",
    "            batch_reviews,\n",
    "            max_length = 256,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt'\n",
    "            )\n",
    "            \n",
    "\n",
    "    # Move inputs of each batch to the same device as the model\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    \n",
    "    # Embedding each batch\n",
    "            #Forward pass\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "            # Mean pooling to get sentence embeddings\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "            # Move to CPU and convert to numpy\n",
    "            batch_embeddings = embeddings.cpu().detach().numpy()\n",
    "        \n",
    "            # Append to all embeddings list\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "\n",
    "\n",
    "    # Ending Timer\n",
    "    total_end_time = time.time()\n",
    "    total_elapsed_time = total_end_time - total_start_time\n",
    "            \n",
    "    print(f\"The total time used by {model_name} model is {total_elapsed_time} seconds, which equals to {total_elapsed_time/60} minutes, and {total_elapsed_time/3600} hours\")\n",
    "            \n",
    "    # Combine all batches\n",
    "    final_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "    \n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1ad58-01ac-4015-b6de-e2b541701165",
   "metadata": {},
   "source": [
    "# Step 4: Experiment for four models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592c74d2-a928-4b4b-ab78-b44f566700a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models we want to use\n",
    "models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"distilbert-base-uncased\", \n",
    "    \"xlm-roberta-base\",\n",
    "    \"google/mt5-base\"\n",
    "]\n",
    "\n",
    "# Dictionary to store all embeddings\n",
    "all_embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7497ce42-1450-4ba0-b8ca-55278f34188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3633/3633 [2:09:41<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time used by bert-base-uncased model is 7782.892159938812 seconds, which equals to 129.71486933231353 minutes, and 2.161914488871892 hours\n",
      "üíæ Saved: embeddings_bert-base-uncased.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3633/3633 [1:03:50<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time used by distilbert-base-uncased model is 3831.9693439006805 seconds, which equals to 63.866155731678006 minutes, and 1.0644359288613001 hours\n",
      "üíæ Saved: embeddings_distilbert-base-uncased.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d975f8cca244929f6d340df639a7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f93cc6825cd4f438c22948284c97596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb4c4b3f1294c4892ac9d88521d4a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedef029665046a6aedc8777dd45c3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ef1e34977246e6a47fbc5fc4495c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3633/3633 [2:06:25<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time used by xlm-roberta-base model is 7914.656589984894 seconds, which equals to 131.9109431664149 minutes, and 2.1985157194402483 hours\n",
      "üíæ Saved: embeddings_xlm-roberta-base.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c6b20d2c8741d5b44ebc231ad83e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2a449b5556428aa55bf57986d8c284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469fd013cc304cb2b46991f873da163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4a1c4a58594064a2e34a3d0c3b527e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89126e6c7234422cb0d58d0381179494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error with google/mt5-base: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    try:\n",
    "        embeddings = tokenization_and_embedding(df, model_name)\n",
    "        all_embeddings_dict[model_name] = embeddings\n",
    "\n",
    "        # Save embeddings\n",
    "        saving_name = model_name.replace(\"/\", \"_\")\n",
    "        np.save(f\"embeddings_{saving_name}.npy\", embeddings)\n",
    "        print(f\"üíæ Saved: embeddings_{saving_name}.npy\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model_name}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5307e4-c22b-48d9-962b-6add0c26510e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
